{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import tensorlayer as tl\n",
    "\n",
    "\n",
    "\n",
    "dataset = '102flowers' #\n",
    "need_256 = True # set to True for stackGAN\n",
    "\n",
    "\n",
    "\n",
    "if dataset == '102flowers':\n",
    "    \"\"\"\n",
    "    images.shape = [8000, 64, 64, 3]\n",
    "    captions_ids = [80000, any]\n",
    "    \"\"\"\n",
    "    cwd = os.getcwd()\n",
    "    img_dir = os.path.join(cwd, '102flowers/102flowers')\n",
    "    caption_dir = os.path.join(cwd, '102flowers/text_c10')\n",
    "    VOC_FIR = cwd + '/vocab.txt'\n",
    "\n",
    "    ## load captions\n",
    "    caption_sub_dir = load_folder_list( caption_dir )\n",
    "    captions_dict = {}\n",
    "    processed_capts = []\n",
    "    for sub_dir in caption_sub_dir: # get caption file list\n",
    "        with tl.ops.suppress_stdout():\n",
    "            files = tl.files.load_file_list(path=sub_dir, regx='^image_[0-9]+\\.txt')\n",
    "            for i, f in enumerate(files):\n",
    "                file_dir = os.path.join(sub_dir, f)\n",
    "                key = int(re.findall('\\d+', f)[0])\n",
    "                t = open(file_dir,'r')\n",
    "                lines = []\n",
    "                for line in t:\n",
    "                    line = preprocess_caption(line)\n",
    "                    lines.append(line)\n",
    "                    processed_capts.append(tl.nlp.process_sentence(line, start_word=\"<S>\", end_word=\"</S>\"))\n",
    "                assert len(lines) == 10, \"Every flower image have 10 captions\"\n",
    "                captions_dict[key] = lines\n",
    "    print(\" * %d x %d captions found \" % (len(captions_dict), len(lines)))\n",
    "\n",
    "    ## build vocab\n",
    "    if not os.path.isfile('vocab.txt'):\n",
    "        _ = tl.nlp.create_vocab(processed_capts, word_counts_output_file=VOC_FIR, min_word_count=1)\n",
    "    else:\n",
    "        print(\"WARNING: vocab.txt already exists\")\n",
    "    vocab = tl.nlp.Vocabulary(VOC_FIR, start_word=\"<S>\", end_word=\"</S>\", unk_word=\"<UNK>\")\n",
    "\n",
    "    ## store all captions ids in list\n",
    "    captions_ids = []\n",
    "    try: # python3\n",
    "        tmp = captions_dict.items()\n",
    "    except: # python3\n",
    "        tmp = captions_dict.iteritems()\n",
    "    for key, value in tmp:\n",
    "        for v in value:\n",
    "            captions_ids.append( [vocab.word_to_id(word) for word in nltk.tokenize.word_tokenize(v)] + [vocab.end_id])  # add END_ID\n",
    "            # print(v)              # prominent purple stigma,petals are white inc olor\n",
    "            # print(captions_ids)   # [[152, 19, 33, 15, 3, 8, 14, 719, 723]]\n",
    "            # exit()\n",
    "    captions_ids = np.asarray(captions_ids)\n",
    "    print(\" * tokenized %d captions\" % len(captions_ids))\n",
    "\n",
    "    ## check\n",
    "    img_capt = captions_dict[1][1]\n",
    "    print(\"img_capt: %s\" % img_capt)\n",
    "    print(\"nltk.tokenize.word_tokenize(img_capt): %s\" % nltk.tokenize.word_tokenize(img_capt))\n",
    "    img_capt_ids = [vocab.word_to_id(word) for word in nltk.tokenize.word_tokenize(img_capt)]#img_capt.split(' ')]\n",
    "    print(\"img_capt_ids: %s\" % img_capt_ids)\n",
    "    print(\"id_to_word: %s\" % [vocab.id_to_word(id) for id in img_capt_ids])\n",
    "\n",
    "    ## load images\n",
    "    with tl.ops.suppress_stdout():  # get image files list\n",
    "        imgs_title_list = sorted(tl.files.load_file_list(path=img_dir, regx='^image_[0-9]+\\.jpg'))\n",
    "    print(\" * %d images found, start loading and resizing ...\" % len(imgs_title_list))\n",
    "    s = time.time()\n",
    "\n",
    "    # time.sleep(10)\n",
    "    # def get_resize_image(name):   # fail\n",
    "    #         img = scipy.misc.imread( os.path.join(img_dir, name) )\n",
    "    #         img = tl.prepro.imresize(img, size=[64, 64])    # (64, 64, 3)\n",
    "    #         img = img.astype(np.float32)\n",
    "    #         return img\n",
    "    # images = tl.prepro.threading_data(imgs_title_list, fn=get_resize_image)\n",
    "    images = []\n",
    "    images_256 = []\n",
    "    for name in imgs_title_list:\n",
    "        # print(name)\n",
    "        img_raw = scipy.misc.imread( os.path.join(img_dir, name) )\n",
    "        img = tl.prepro.imresize(img_raw, size=[64, 64])    # (64, 64, 3)\n",
    "        img = img.astype(np.float32)\n",
    "        images.append(img)\n",
    "        if need_256:\n",
    "            img = tl.prepro.imresize(img_raw, size=[256, 256]) # (256, 256, 3)\n",
    "            img = img.astype(np.float32)\n",
    "\n",
    "            images_256.append(img)\n",
    "    # images = np.array(images)\n",
    "    # images_256 = np.array(images_256)\n",
    "    print(\" * loading and resizing took %ss\" % (time.time()-s))\n",
    "\n",
    "    n_images = len(captions_dict)\n",
    "    n_captions = len(captions_ids)\n",
    "    n_captions_per_image = len(lines) # 10\n",
    "\n",
    "    print(\"n_captions: %d n_images: %d n_captions_per_image: %d\" % (n_captions, n_images, n_captions_per_image))\n",
    "\n",
    "    captions_ids_train, captions_ids_test = captions_ids[: 8000*n_captions_per_image], captions_ids[8000*n_captions_per_image :]\n",
    "    images_train, images_test = images[:8000], images[8000:]\n",
    "    if need_256:\n",
    "        images_train_256, images_test_256 = images_256[:8000], images_256[8000:]\n",
    "    n_images_train = len(images_train)\n",
    "    n_images_test = len(images_test)\n",
    "    n_captions_train = len(captions_ids_train)\n",
    "    n_captions_test = len(captions_ids_test)\n",
    "    print(\"n_images_train:%d n_captions_train:%d\" % (n_images_train, n_captions_train))\n",
    "    print(\"n_images_test:%d  n_captions_test:%d\" % (n_images_test, n_captions_test))\n",
    "\n",
    "    ## check test image\n",
    "    # idexs = get_random_int(min=0, max=n_captions_test-1, number=64)\n",
    "    # temp_test_capt = captions_ids_test[idexs]\n",
    "    # for idx, ids in enumerate(temp_test_capt):\n",
    "    #     print(\"%d %s\" % (idx, [vocab.id_to_word(id) for id in ids]))\n",
    "    # temp_test_img = images_train[np.floor(np.asarray(idexs).astype('float')/n_captions_per_image).astype('int')]\n",
    "    # save_images(temp_test_img, [8, 8], 'temp_test_img.png')\n",
    "    # exit()\n",
    "\n",
    "    # ## check the first example\n",
    "    # tl.visualize.frame(I=images[0], second=5, saveable=True, name='temp', cmap=None)\n",
    "    # for cap in captions_dict[1]:\n",
    "    #     print(cap)\n",
    "    # print(captions_ids[0:10])\n",
    "    # for ids in captions_ids[0:10]:\n",
    "    #     print([vocab.id_to_word(id) for id in ids])\n",
    "    # print_dict(captions_dict)\n",
    "\n",
    "    # ## generate a random batch\n",
    "    # batch_size = 64\n",
    "    # idexs = get_random_int(0, n_captions_test, batch_size)\n",
    "    # # idexs = [i for i in range(0,100)]\n",
    "    # print(idexs)\n",
    "    # b_seqs = captions_ids_test[idexs]\n",
    "    # b_images = images_test[np.floor(np.asarray(idexs).astype('float')/n_captions_per_image).astype('int')]\n",
    "    # print(\"before padding %s\" % b_seqs)\n",
    "    # b_seqs = tl.prepro.pad_sequences(b_seqs, padding='post')\n",
    "    # print(\"after padding %s\" % b_seqs)\n",
    "    # # print(input_images.shape)   # (64, 64, 64, 3)\n",
    "    # for ids in b_seqs:\n",
    "    #     print([vocab.id_to_word(id) for id in ids])\n",
    "    # print(np.max(b_images), np.min(b_images), b_images.shape)\n",
    "    # from utils import *\n",
    "    # save_images(b_images, [8, 8], 'temp2.png')\n",
    "    # # tl.visualize.images2d(b_images, second=5, saveable=True, name='temp2')\n",
    "    # exit()\n",
    "\n",
    "import pickle\n",
    "def save_all(targets, file):\n",
    "    with open(file, 'wb') as f:\n",
    "        pickle.dump(targets, f)\n",
    "\n",
    "save_all(vocab, '_vocab.pickle')\n",
    "save_all((images_train_256, images_train), '_image_train.pickle')\n",
    "save_all((images_test_256, images_test), '_image_test.pickle')\n",
    "save_all((n_captions_train, n_captions_test, n_captions_per_image, n_images_train, n_images_test), '_n.pickle')\n",
    "save_all((captions_ids_train, captions_ids_test), '_caption.pickle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
